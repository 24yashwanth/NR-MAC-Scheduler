import numpy as np
import torch
import torch.nn as nn
from numpy import array
import torch.optim as optim

import math


# n_steps = [10,50]


# split a univariate sequence
def split_sequence(sequence, n_steps):
	X, y = list(), list()
	for i in range(len(sequence)):
		# find the end of this pattern
		end_ix = i + n_steps
		# check if we are beyond the sequence
		if end_ix > len(sequence)-1:
			break
		# gather input and output parts of the pattern
		seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]

		X.append(seq_x)
		y.append(seq_y)
	return array(X), array(y)

# define input sequence
raw_seq = [2956,3311,3554,3444,3474,2769,2496,2200,2268,1975,1988,1839,2100,2306,3236,3178,3007,2937,3167,3218,3349,3161,3140,3052,3087,3042,2983,2617,2231,2384,1520,1400,1628,1127,1412,1751,2339,1009,472,1055,1318,701,705,531,1707,2385,2383,3068,2743,2966,3015,2851,2927,3096,2952,2404,2108,1878,1862,1640,1992,1454,1488,1288,3349,3540,5001,4882,5341,5324,5301,5305,5326,5441,5417,5424,5441,5431,5317,5286,5184,5057,5302,5377,5522,5291,5314,5355,5391,5412,5328,5324,5260,5239,5089,5145,5282,5147,5269,5097,5275,5266,5207,5370,5169,5001,4548,5063,5235,5070,5832,5869,5784,5689,5755,5793,5815,5733,5660,5666,5671,5691,5697,5716,5725,5729,5749,5730,5782,5677,5674,5656,5702,5729,5650,5573,5683,5685,5676,5651,5670,5623,5590,5562,5540,5614,5545,5453,5426,5338,5391,5433,5439,5407,5333,5326,5289,5315,4841,4696,4317,4530,3973,2728,3094,3740,3770,3857,3819,4110,3349,3852,4254,4326,3778,4038,3795,3471,3397,3522,3837,3785,3992,3934,4388,4367,4715,5027,5130,5092,5212,5292,5311,5314,5073,4869,5190,4708,4624,4582,4524,4294,3598,4081,4307,3736,3284,3212,3290,3058,2775,2890,2589,2529,2949,2928,2652,2639,3095,3459,3287,4747,4426,4336,4042,3976,3666,4305,3866,4103,4393,4347,3606,3245,3462,2836,2467,2483,2247,2486,4574,5117,4399,3228,3202,2790,3374,3709,3400,4015,4411,4316,5158,5349,5244,5287,5352,5437,5286,5450,5270,4993,4922,5373,4661,2191,1618,2013,4406,4053,3500,1752,3587,4536,4820,4618,5168,5465,4285,5678,5712,5389,5377,4899,4873,4847,5213,4526,3941,3916,3928,4010,3960,3652,2962,2632,2469,2178,2637,5471,5516,5473,5615,6093,6125,6071,6081,5002,2924,6060,6089,6094,5813,5626,5890,5432,3936,2666,2352,2028,2116,4267,6125,6081,6094,6090,6109,6101,6079,6022,6004,5980,5970,4936,3602,2583,2235,1735,1599,1178,844,2698,3573,4414,4779,5747,5989,6134,6170,6181,6184,6169,6161,6175,6096,6081,6004,5974,6043,5992,5919,5925,3542,5061,6159,5886,6148,6098,5870,6019,6167,6184,6130,6088,6136,5985,6116,5933,6002,6122,6183,6211,6180,6211,6018,6092,5668,5797,5814,5301,5070,5366,4073,2671,2547,2417,2506,2301,5102,6027,6174,6157,6137,6072,6116,6086,5925,276,5127,5983,5984,6015,6005,6028,6018,5973,3376,6088,6079,5965,6008,6003,5939,2593,5327,5643,5773,5409,5273,3348,6102,6089,6112,6109,6113,6115,6110,6104,6106,6122,6121,6108,6113,6112,6123,6121,6113,6115,6115,6102,6106,6114,6121,6076,6123,6075,6103,6118,6110,6103,6119,6110,6124,6120,6098,6105,6106,6122,6117,6090,6086,6042,6048,6077,6046,6087,5772,6166,6109,6049,6070,6115,4507,4658,5696,5704,5680,5639,5669,5683,5706,5763,5682,5762,5702,5720,5765,5767,5682,5746,5737,5635,5763,5111,5592,5622,5627,5634,5622,5638,5542,5634,5529,5588,5641,5582,5573,5537,5462,5404,5119,5164,4989,4973,4780,3798,2972,2550,2094,2697,2562,2376,2544,2375,2493,2496,2370,2404,2299,2550,2369,2526,2475,2267,2559,2293,2453,2412,3898,5561,5532,5545,5399,5578,2523,2619,2499,2529,2613,2246,1693,3215,2844,2798,2680,2091,1729,1666,1772,1400,1347,1469,1856,2203,1828,2089,1673,1114,1063,1425,1360,1086,1380,1497,1952,2759,3774,3924,4902,4883,4761,5073,4823,5234,4923,5281,5304,5297,4877,5337,4709,4721,5176,5174,4632,4878,4516,4900,4668,4367,3972,4514,4516,4535,3777,3311,2078,2160,2082,1592,1539,1530,2207,2375,1923,1756,1489,1751,1712,2199,3486,3597,3649,4135,2866,2251,1727,1537,1461,1471,1660,1652,1612,1717,1692,1654,1671,2231,2721,3089,3898,4031,3570,3605,3161,3117,2831,2594,2777,2717,2776,2828,2887,2892,2604,2368,1895,1497,1151,1568,1867,2173,2013,2022,2331,2650,2763,2247,2272,2448,2423,2057,1949,1803,1721,1736,1523,1373,1505,1429,1197,1530,1563,1343,792,410,1953,1986,2012,1955,2089,2073,2219,2000,2064,2118,2202,2194,2090,2131,2280,2279,2191,2209,2062,2162,2236,2181,2182,2095,2174,2194,1892,1926,1658,1902,2201,2197,2008,2096,2215,2200,2175,2320,2293,2457,2293,2344,2413,2292,2227,2466,2296,2393,2342,2459,2320,2331,2302,2382,2311,2306,2314,2274,2350,2355,2243,2364,2341,2411,2396,2324,2332,2269,2507,2451,2523,2524,4325,3895,4226,4890,4631,4597,4796,3635,3146,3673,3780,2718,2697,2652,2716,2689,2657,2721,2721,2730,2715,2748,2756,2672,2646,2677,2707,2767,2641,2706,2775,2717,2634,2716,2685,2694,2637,2679,2481,2159,2465,2369,2404,2465,2445,2442,3556,4831,4495,2578,2743,2777,2759,2749,2803,2792,2759,2723,2808,2857,2831,2788,2802,2837,2835,2808,2810,2801,2826,2852,2873,2762,2777,2743,2690,2776,2826,2821,2813,2780,2733,2789,2775,2829,2832,2722,2790,2812,2673,2831,2817,2804,2707,2573,2831,2816,2758,2783,2786,2796,2801,2815,2827,2872,2829,2810,2826,2684,2786,2791,2808,2754,2784,2814,2835,2789,2800,2815,2803,2827,2829,2762,2746,2812,2806,2798,2734,2711,2731,2769,2810,2836,2815,2827,2828,2739,2584,2803,2766,2828,2786,2780,2836,2837,2784,3099,5153,5363,5629,5661,4766,2552,2710,2713,2736,2709,2769,2789,2588,2651,2588,2635,2611,2594,2531,2698,2771,2800,2745,2810,2716,2720,2732,2968,5417,5518,5385,5554,5575,5438,5488,5412,5474,5580,5525,5640,5656,5668,5652,5627,5512,5423,5621,5639,5590,5450,5566,5508,5563,5641,5650,5583,5593,5655,5139,5189,5072,5158,5058,5054,5145,5080,4973,4903,5147,5105,5188,5164,5306,5118,5142,5386,5517,5510,5563,5573,5447,5152,5417,5513,5553,5494,5591,5475,5614,5595,5555,4856,5466,4769,5402,5408,5329,5521,5611,5657,5669,5681,5532,5683,5667,5552,5499,5708,5633,5656,5589,5605,5707,5694,5637,5549,5536,5622,5562,5393,4039,2766,2720,2741,2677,3004,2942,2751,2737,2778,2793,2820,2727,2831,2734,2766,2804,2709,2650,3815,5332,5380,5487,5528,5510,5473,5373,5339,5170,3672,3106,3578,3795,3554,3181,3235,3215,1946,2114,1943,1880,1970,2751,2748,2770,2696,2728,2730,2683,2640,2780,2782,2826,2805,2796,2739,2814,2764,2895,2848,2780,2774,2827,2765,2787,2810,2786,2807,2726,2819,2784,2812,2759,2749,2758,2667,2739,2725,2670,2727,2685,2599,2556,2593,2590,2649,2676,2656,2660,2816,2652,4669,4801,4273,3975,3634,3430,2170,2594,2689,2750,2734,2719,2715,2653,2768,2784,2786,2708,2709,2689,2611,2685,2533,2301,2054,1841,1617,1640,1649,1776,1736,1723,1754,1731,1771,1722,1744,1709,1778,1769,1744,1735,1764,1701,1719,1643,1817,1811,1755,1728,1782,1723,1581,1497,2982,4321,3575,3186,2986,2907,2991,4260,4540,3196,3021,2978,2952,2892,2968,2998,2970,3023,3010,3011,3002,2930,2942,2943,2967,3049,5946,5895,5860,5895,5802,5950,5906,5930,5928,5975,5940,5848,4711,2953,2902,2891,2916,2894,2891,2968,2889,2897,2926,2901,2950,2977,2968,2961,2971,2937,2922,2914,2893,2831,5197,5865,5835,5795,5565,5474,4984,2812,2745,2764,2758,2747,2720,2745,2729,2758,2706,2619,2729,2698,2708,3634,5040,5540,5481,5504,5525,5567,5603,5545,5609,5601,5601,5511,5503,5605,5622,5608,5736,5637,5711,5682,5765,5721,5737,5765,5659,5628,5701,5716,5618,5029,5366,5617,5494,5684,5507,4278,1695,2594,4265,3376,2765,2818,2828,2871,2848,2844,2832,2831,2752,2832,2857,2795,4592,5614,5675,5421,5553,5607,5502,5204,5621,5623,5763,5687,5757,5693,4972,2796,2689,2790,2783,2861,2870,2869,2866,2858,2868,2873,2858,2846,2871,2872,2854,2885,2864,2843,2788,2756,2841,2805,2794,2791,2821,2797,2832,2833,2859,2873,2838,2829,2848,2854,2832,2843,2836,2858,2858,2861,2818,2853,2845,2860,2861,2845,2866,2863,2867,2762,2831,2802,2826,2857,2860,2805,2776,2829,2813,2789,2813,2815,2744,2772,2827,2836,2864,2825,2826,2828,2840,2852,2803,2793,2811,2792,2725,2797,2822,2796,2830,2816,2839,2832,2805,2736,2739,2726,2826,2794,2610,2776,2782,2837,2817,2829,2800,2781,2838,2853,2792,2820,2803,2842,2795,2734,2749,2709,2707,2745,2649,2746,2723,2768,2761,2727,2742,2718,2752,2808,2692,2710,2711,2731,2781,2764,2766,2815,2816,2755,2791,2815,2726,2751,2824,2771,2801,2782,2840,2842,2754,2810,2852,2794,2828,2803,2804,2750,2791,2830,2852,2867,2843,2876,2875,2758,2779,2813,2755,2797,2719,2544,2672,2770,2716,2765,2742,2796,2870,2734,2805,2794,2738,2728,2696,3309,5630,5324,5400,5506,4722,5112,4756,4878,4399,3917,3636,3811,3750,2136,2593,2761,2790,2766,2874,2820,2855,2803,2788,2828,2806,1859,1786,1854,1951,2838,2848,2775,2835,2851,2839,2823,2864,2838,2849,2846,2859,2892,2847,2812,2782,2832,2799,2831,2804,2848,2761,2837,2846,2846,2768,2823,2798,2828,2792,2837,2811,2874,2869,2893,2891,2877,2896,2868,2831,2875,2858,2841,2896,2849,2735,2863,2811,2852,2859,2848,2869,2895,2891,2872,2847,2875,2865,2884,2891,2874,2889,2880,2892,2903,2902,2907,2887,2879,2893,2887,2831,2852,2864,2772,2839,2872,2813,2842,2834,2843,2847,2888,2901,2877,2874,2899,2885,2858,2854,2791,2848,2846,2883,2830,2881,2860,2860,2883,2851,2826,2826,2869,2873,2867,2837,2848,2872,2835,2817,2874,2806,2817,2829,2828,2832,2826,2766,2875,2863,2859,2842,2849,2851,2867,2825,2846,2888,2890,2861,2906,2908,2880,2881,2856,2867,2869,2862,2875,2879,2879,2843,2860,2860,2830,2825,2831,2822,2867,2855,2858,2865,2852,2867,2805,2855,2858,2862,2883,2900,2912,2896,2924,2846,2876,2863,2845,2857,2814,2798,2824,2765,2412,2391,2253,2242,2282,2633,3394,4718,4330,3903,3266,2445,1977,1820,1678,1586,1477,698,967,874,1266,1042,1200,1229,1303,1055,881,709,806,760,1040,963,1004,809,967,1119,1036,2296,2734,1454,1167,1861,1631,1482,1488,975,1376,1577,1324,1630,1733,1999,1748,1274,1944,1211,788,777,1411,2290,3075,3008,3292,3185,3155,3297,3346,4106,4095,3904,4129,3704,3589,3908,3203,2831,3632,4299,4199,4492,4635,4915,4667,4327,4600,4534,4751,4527,4530,4675,4666,4641,4688,4451,4435,4194,4066,2785,2111,2042,2072,1812,1648,1687,1782,2117,1721,1711,1463,1461,1571,1615,1694,1585,1428,1538,1276,1336,1393,2310,4509,4405,4239,3804,2677,2022,1297,1263,1870,1660,1718,1864,1952,1862,1848,1802,1500,1311,1395,1749,1646,1440,1289,1248,965,911,940,912,934,934,743,871,930,771,939,911,1280,2077,2203,2393,2817,3275,3550,1909,917,692,2680,2603,2093,2687,2069,2376,2196,3605,3326,2260,4677,4187,3613,3242,3265,1622,2268,2055,1702,1766,2133,2428,2491,2348,2953,2660,2931,2608,2968,2552,2710,2681,2632,2630,2517,2617,2621,2510,2686,2613,2535,2569,2468,2598,2522,2587,2668,2611,2679,2611,2574,2650,2618,2525,2551,2755,2662,2712,2698,2600,2458,2369,2444,2516,2576,2756,2661,2642,2501,2390,2373,2340,2360,1940,2370,2536,2266,2491,2179,2303,2314,2230,2353,2329,1974,1990,2424,2474,2444,2500,2562,2577,2551,2571,2608,2629,2690,2648,2709,2651,2591,2632,2613,2642,2555,2649,2678,2595,2590,2691,2641,2725,2626,2671,2665,2634,2708,2724,2703,2722,2765,2764,2567,2644,2675,2746,2644,2669,2631,2650,2636,2601,2575,2592,2628,2647,2735,2667,2624,2676,2549,2664,2722,2710,2603,2646,2715,2779,2739,2693,2652,2690,2790,2701,2727,2823,2748,2683,2692,2759,2645,2631,2621,2668,2645,2563,2647,2544,2576,2650,4894,5098,4949,4975,4878,4292,2594,2479,2596,2611,2420,2284,2521,3387,4168,4207,4157,4165,4447,4721,2338,2009,1993,2148,2233,2135,2156,1982,1856,1850,2066,2035,1957,1833,1875,2014,1992,1926,2009,2033,2073,2271,2182,2156,2184,2189,2084,2256,2310,2207,1981,2103,2156,2061,2153,1939,2009,1956,2119,2076,2120,2106,2165,2075,2051,2002,1862,1874,1600,1733,1695,2143,2237,2093,2078,2172,2166,2219,2452,2082,2074,1908,1773,1387,2786,3467,5163,5252,5656,5524,4343,1546,539,587,409,638,740,716,845,1503,2054,2320,2639,2741,2757,2776,2684,2644,2725,2701,2733,2720,2695,2644,2642,2625,2598,3367,5207,5273,4717,4602,3501,1629,1555,1564,1660,2163,1505,1893,1936,2111,2208,2171,2736,3375,3976,3492,3274,2021,2286,4522,3304,3039,2190,2290,2108,2800,2678,5173,3965,3471,4253,2941,1911,2596,2544,2559,2360,2559,2458,2595,2610,2546,2463,2549,2464,2568,2462,2396,2448,2408,2518,2512,2469,2575,2648,2649,2556,2498,2438,2504,2551,2466,2536,2557,2659,2615,2609,2607,2595,2633,2621,2636,2502,2673,2683,2671,2662,2651,2516,2592,2647,2705,2727,2647,2594,2634,2593,2607,2660,2610,2612,2714,2681,2739,2689,2695,2695,2725,2703,2715,2688,2650,2699,2674,2752,2653,2632,2610,2676,2221,2450,2689,2599,2623,2584,2657,2651,2628,2646,2726,2711,2784,2766,2668,2435,2559,2511,2630,2580,2594,2619,2591,2620,2661,2687,2592,2663,2728,2729,2762,2648,2700,2695,2446,2622,2646,2656,2672,2716,2734,2706,2598,2591,2667,2570,2658,2737,2706,2734,2779,2713,2695,2700,2764,2752,2680,2694,2707,2796,2650,2773,2688,2687,2680,2665,2693,2719,2684,2766,2740,2832,2750,2788,2705,2786,4953,4756,4467,3015,2845,2735,2681,2688,2691,2645,2709,2673,2702,2745,2714,2722,2704,2645,2641,2775,2699,2796,2759,2787,2750,2763,2741,2788,2778,2779,2783,2753,2779,2718,2750,2719,2751,2691,2756,2689,2720,2722,2723,2729,2735,2745,2755,2768,2765,2754,2784,2740,2708,2716,2754,2733,2702,2655,2659,2707,2623,2692,2627,2643,2989,4230,4430,3806,2903,2689,2755,2742,2769,2778,2762,2757,2764,2761,2748,2705,2700,2723,2711,2720,2714,2767,2673,2738,2674,2647,2685,2647,2711,2637,2617,2671,2687,2727,2739,2706,2644,2701,2681,2688,2615,2665,2622,2692,2702,2660,2664,2576,2581,2625,2615,2590,2293,2375,2409,2531,2487,2558,2560,2531,2450,2521,2427,2471,2413,2408,2301,2370,2118,2117,2004,2178,2126,2409,2429,2214,2128,1972,1991,2382,3401,3607,3464,3632,3792,3383,3386,2684,2864,2641,2952,2868,2973,3053,3075,3311,3246,3331,3411,3333,2944,2748,2765,2343,1940,1951,1546,1430,1370,1390,1869,1976,2065,2365,2231,2242,2092,2597,2533,2662,2812,3274,3708,4409,4475,4327,3907,4109,4105,4245,3451,3352,3091,3263,3155,3411,3017,3665,3572,3735,3706,3181,3401,3180,3483,3441,3831,3908,4060,3242,2857,2896,2839,2774,2857,2865,2855,2850,2866,2873,2870,2873,2845,2886,2888,2827,2883,2895,2787,2805,2765,2812,2815,2918,2904,2961,2940,2891,2917,2878,2892,2751,2865,2857,2878,2898,2861,2864,2849,2841,2860,2780,2792,2797,2724,5142,5825,5565,5294,5257,5525,5103,5622,5214,5389,5147,5621,5749,5885,5948,5761,5893,5793,5851,5309,5527,5308,4969,4940,5596,5469,5521,5899,5891,5848,5861,5855,5822,5748,5303,5645,5739,4886,2730,2879,2881,2874,2863,2896,2898,2853,2866,2867,2884,2875,2877,2873,2873,2886,2855,2884,2876,2871,2862,2868,2860,2871,2863,2874,2873,2872,2875,2877,2815,2844,2838,2840,2853,2847,2856,2832,2835,2851,2852,2854,2858,2855,2857,2896,2855,2868,2867,2848,2854,2861,2868,2828,2862,2793,2820,2838,3079,5385,5444,5712,5593,5714,5619,5748,5838,5831,5834,5870,5815,5793,5820,5927,5941,5861,5898,5812,5920,5936,5925,5918,5894,5797,5833,5893,5753,5306,5852,5764,5811,5850,5799,5864,5875,5864,5773,5867,5871,5848,5882,5892,5917,5849,5729,5726,5735,5801,5895,5814,5616,4661,5430,5573,5746,5752,5902,5954,5962,5955,5913,5737,5924,5805,5676,5544,5466,5457,5443,5946,5638,5649,5629,5737,5726,5976,5965,5968,5929,5972,5968,5967,5967,5935,5943,5863,5868,5931,5946,5953,5930,5927,5414,4966,5087,4650,5818,5904,5863,5928,5905,5837,5876,5757,5062,4076,2658,2644,2416,2537,2610,2556,2613,2650,2540,2567,2571,2596,2607,2706,2681,2657,2648,2649,2533,2568,2597,2591,2580,2515,2481,2539,2501,2599,2508,2555,2611,2573,2648,2606,2589,2627,2592,2634,2647,2678,2646,2590,2607,4960,5113,4990,4573,4064,4597,4528,4914,4783,4557,4893,4813,2868,2735,2858,5313,5334,5390,3092,2876,2820,2809,2814,2881,2851,2809,2811,2804,2848,2840,2852,2826,2853,2873,2872,2905,2832,2894,2859,2872,2797,2803,2839,2863,2863,2843,2839,2774,2775,2725,2797,2713,2705,2643,4860,5342,5784,5689,5651,5843,5745,5672,5195,4973,4564,4871,5075,5101,5343,4708,5275,5603,5669,5588,4236,2574,2535,3187,5177,5476,5385,5353,5560,5376,5464,5293,5464,5491,5446,5604,5364,5319,5300,5171,4874,5111,4797,4676,4734,4691,4257,4087,4235,4683,4617,4434,4551,4578,4538,4396,3713,4077,3896,4006,4223,4243,4021,3823,4348,4372,4373,4472,4412,4323,4363,4364,4348,4347,4171,4370,4406,4372,4218,4404,4245,4320,4328,4579,4579,4309,4622,4821,4988,5255,5410,5499,5514,5358,5798,5814,5804,5820,5803,5690,5689,4870,2489,2725,2698,2653,2599,2750,2527,2616,2622,2681,2685,2580,2630,2602,2462,2668,2693,2632,2613,2592,2545,2524,2324,2223,2398,2534,2498,2515,2528,2546,2625,2438,2472,2290,2517,2580,3023,4698,4665,5033,4672,4693,4343,4491,4399,4084,3049,2856,2874,2870,2833,2908,2873,2856,2786,2855,2865,2861,2809,2768,2837,2870,2935,3005,2962,2861,2977,2985,2843,2826,2771,3160,5752,5805,5789,5812,5884,5827,5535,5262,5219,4821,4792,3967,4507,3648,2030,2271,2354,2308,2364,2501,2418,2358,2501,2567,2539,2688,2530,2645,2554,2883,2906,2755,2743,2801,2825,2755,2478,2723,2732,2753,2815,2692,2539,2746,2451,2672,2764,2697,2717,2703,2741,2606,2693,2477,4556,4504,3382,4063,4698,4858,4835,4669,4107,3829,2680,2287,1522,2793,2748,2785,2689,2715,2709,2572,2668,2635,2706,2710,2676,2682,2686,2681,2680,2724,1940,1708,2019,1509,1192,1685,1877,2518,2647,2194,5441,5449,5605,5520,5213,5192,5328,5538,5278,5351,4495,4902,4935,5195,4926,4819,5080,5194,5400,4640,3512,3852,1375,2734,2792,2835,3162,5795,5818,5822,5815,5824,5795,5874,5856,5144,5228,5515,5871,5909,4674,2990,2851,2814,2894,2804,2898,2805,2739,2273,2781,2341,4280,5084,4231,4283,3098,2673,2353,2773,2669,2928,2622,2436,2796,2787,2516,2830,2808,2780,2742,3180,4812,5336,5573,5833,5697,5646,2947,2893,2958,2877,2918,2855,2929,2897,2184,5468,5645,5601,5454,5089,3073,3653,3589,2808,2090,2628,2821,2860,2869,2556,2827,2797,4184,4930,4832,4818,5342,2970,70,3877,4303,3836,3304,4597,5538,5810,3410,1270,5716,5526,5504,5518,3356,5551,5533,5518,5721,5652,5563,5231,4379,5350,5572,5637,5637,2408,4755,5252,5528,5677,5449,5632,5718,5770,5825,5840,5839,5821,5831,5845,5832,5826,5831,5828,5836,5848,5838,5845,5833,5774,5803,5814,5785,5417,4301,2972,2839,2706,2765,4742,5593,5547,5696,5581,5152,736,2221,2755,2816,2823,2837,2792,2823,2810,2782,2790,2816,2829,2848,2831,2845,2827,2845,2822,2820,2810,2831,2829,2848,2845,2784,2825,2826,2811,2858,2849,2836,2815,2856,2826,2767,2756,2691,2601,2615,2709,3064,5059,4478,4140,4456,4360,2662,2475,2603,2652,2674,2602,2644,2654,2549,2587,2744,2597,2647,2633,2601,2482,2490,2579,2609,2629,2541,2640,2486,2263,2459,2359,2302,2477,2581,2529,2625,2344,2324,2419,2422,2344,2360,2194,2164,2181,2128,1994,1997,1936,2128,2205,2112,2100,2333,2334,2416,3982,2907,2540,3337,3284,2484,2478,2687,2038,1702,1765,1809,1846,2128,2392,2779,2644,2815,2794,2827,2748,2790,2792,2694,2729,2754,2762,2796,2733,2738,2661,2668,2628,2675,2648,2746,2736,2767,2743,2771,2723,2786,2785,2775,2743,2724,2719,2675,2745,2696,2751,2716,2677,2724,2730,2707,2764,2739,2736,2721,2649,2723,2649,2662,2586,2483,2522,2646,2606,2607,2423,2514,3658,3214,2176,2394,2974,2547,1963,1109,1582,976,1061,1027,936,1040,1117,1161,1010,1068,929,1011,854,575,599,741,553,595,578,562,472,609,582,553,589,661,807,931,941,1119,1064,1465,878,727,785,808,828,810,778,608,564,809,580,682,709,701,599,627,713,1085,1207,1162,1255,1195,1018,917,1011,989,785,761,775,762,887,1164,980,827,1070,921,1054,1115,996,522,1896,1911,1887,1883,2010,1828,2109,2140,2233,2259,2202,2219,2200,1966,2085,2228,2370,2198,2179,2050,2037,2021,2086,2076,2160,2270,2262,2233,2253,2399,2497,2452,2571,2519,2418,2223,2538,2645,2585,2492,2459,2465,2479,2436,2370,2494,2435,2464,2358,2407,2360,2441,2368,2409,2444,2317,2352,2439,2433,2473,2522,2564,2628,2597,2418,2522,2582,2477,2613,2525,2582,2350,2434,2343,2294,2348,2379,2417,2383,2408,2349,2381,2300,2229,2178,2244,2274,2130,2222,2314,2320,2339,2312,2300,2328,2337,2253,2255,2300,2334,2296,2292,2435,2488,2481,2490,2396,2071,2819,3865,4083,4055,4443,4063,3879,3840,4148,4133,3633,2644,2481,2488,2510,2428,2413,2402,2376,2475,2456,2468,2486,2432,2475,2479,2378,2434,2425,2420,2473,2497,2445,2493,3350,4174,4218,4249,4186,4171,3693,4294,4021,3916,3966,3971,4161,4172,3107,2792,2798,2823,2730,2844,2754,2733,2782,2754,2850,2507,2777,2643,2761,2743,2633,2597,2718,2717,2661,2708,2573,2681,2569,2713,2684,2678,2813,2713,2734,2716,2754,2704,2657,2759,2699,2657,2793,2676,2515,2808,2698,2737,2762,2779,2607,2446,2638,2456,2606,2666,2650,2652,2819,2788,2734,2653,2719,2790,2782,2781,2761,2778,2785,2795,2729,2677,2741,2782,2802,2816,2817,2825,2806,2801,2757,2768,2742,2739,2802,2764,2755,3066,5016,5425,5574,5436,5421,4370,2861,2735,2785,2803,2831,2707,2822,2801,2666,2692,2754,2780,2771,2812,2675,2786,2782,2735,2794,2819,2741,2826,2801,2776,2724,2695,2697,2804,2824,2761,2764,2707,2758,2786,2762,2777,2683,2699,2761,2701,2720,2776,2750,2696,2695,2719,2817,2800,2708,2731,2717,2789,2727,2776,2665,2631,2358,2424,2272,2418,2514,2568,2418,2579,2718,2684,2658,2667,2610,2674,2654,2637,2683,2647,2663,2588,2458,2631,2556,2555,2527,2575,2539,2608,2516,2641,5192,5022,5379,5384,5167,4908,5106,5444,5384,5396,5568,5531,5515,5480,5383,5469,5517,5387,5402,5234,5369,5124,5242,4865,5351,5510,5267,5115,4719,5254,5238,5289,5537,5428,5233,5514,5420,5490,5413,5330,5472,5392,4852,5373,5428]

raw_seq = [np.float32(item) for item in raw_seq]

def train_test_split(X,Y, value):
	X_train, X_test, Y_train, Y_test = list(),list(),list(),list()
	for i in range(int((1-value)*len(X))):
		X_train.append(X[i])
		Y_train.append(Y[i])
	for i in range(int((1-value)*len(X)), len(X)):
		X_test.append(X[i])
		Y_test.append(Y[i])
	return array(X_train), array(Y_train), array(X_test), array(Y_test)


def create_train_model(n_steps, raw_seq):
	# split into samples
	X, Y = split_sequence(raw_seq, n_steps)
	n_features = 1
	X_train, Y_train, X_test, Y_test = train_test_split(X,Y, 0.03)

n_steps = 10 //10,50
X, Y = split_sequence(raw_seq, n_steps)
X_train, Y_train, X_test, Y_test = train_test_split(X,Y, 0.03)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.register_buffer('pe', self._get_positional_encoding(max_len, d_model))

    def _get_positional_encoding(self, max_len, d_model):
        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        return pe.unsqueeze(0)

    def forward(self, x):
        # Calculate the positional encoding for the input sequence x
        pe = self.pe[:, :x.size(0)]
        x = x + pe  # Broadcast pe to match the dimensions of x
        return x

class TransformerModel(nn.Module):
    def __init__(self, input_dim, nhead, num_layers, d_model, ff_dim, dropout_rate, output_dim):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Linear(input_dim, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=ff_dim, dropout=dropout_rate)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc = nn.Linear(d_model, output_dim)

    def forward(self, x):
        x = self.embedding(x)
        x = self.pos_encoder(x)
        x = self.transformer_encoder(x)
        x = torch.mean(x, dim=1)  # Aggregate sequence information
        x = self.fc(x)
        return x

# Hyperparameters
input_dim = n_steps  # Input dimension (sequence length)
num_blocks = 2  # Number of transformer blocks //2,4
d_model = 64    # Dimension of the model //32,64,128
num_heads = 4   # Number of attention heads //8,4
ff_dim = 32     # Dimension of the feedforward network //16,32,64
dropout_rate = 0.1  # Dropout rate //0.001-0.9
output_dim = 1  # Output dimension (for regression)

# Create the Transformer model
model = TransformerModel(input_dim=input_dim, nhead=num_heads, d_model=d_model, num_layers=num_blocks, ff_dim=ff_dim, dropout_rate=dropout_rate, output_dim=output_dim)
# model.float()  # Ensure the model is in float mode



# Display model architecture
print(model)

input_data = torch.tensor(X_train, dtype=torch.float32)
target_data = torch.tensor(Y_train, dtype=torch.float32)

testing_input_data = torch.tensor(X_test, dtype=torch.float32)
testing_target_data = torch.tensor(Y_test, dtype=torch.float32)

# Define loss function (e.g., Mean Squared Error for regression)
criterion = nn.MSELoss()

# Define optimizer (e.g., Adam)
optimizer = optim.Adam(model.parameters(), lr=0.001) // 0.5,0.1,0.01,0.001


# Number of training epochs
num_epochs = 150 //500, 1000

# Training loop
for epoch in range(num_epochs):
    model.train()  # Set the model to training mode

    # Forward pass
    outputs = model(input_data)  # input_data is your input tensor

    # Compute the loss
    loss = criterion(outputs, target_data)  # target_data is your ground truth tensor

    # Backpropagation and optimization
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Print training progress
    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')

filename = './model/Transformers_BSR_model.pth'
# After training, Saving the model
torch.save(model.state_dict(), filename)


loaded_model = TransformerModel(input_dim=input_dim, nhead=num_heads, d_model=d_model, num_layers=num_blocks, ff_dim=ff_dim, dropout_rate=dropout_rate, output_dim=output_dim)  # Initialize a new instance of your model
loaded_model.load_state_dict(torch.load('./model/Transformers_BSR_model.pth'))
loaded_model.eval()

with torch.no_grad():
    predictions = loaded_model(testing_input_data)

# Calculate loss (e.g., Mean Squared Error) for regression
criterion = torch.nn.MSELoss()
loss = criterion(predictions, testing_target_data)

print("Test Loss:", loss.item())
